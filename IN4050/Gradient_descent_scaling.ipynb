{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: The effect of scaling for gradient descent\n",
    "The first part was already in the notebook on Linear Regression. we here extending it with scaling.\n",
    "\n",
    "We see that we may use a larger learner rate and consequently fewer epochs by scaling.\n",
    "\n",
    "We also plot some contour lines and observe the effect of these lines from scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with some random points\n",
    "points=[\n",
    "    (1,10),\n",
    "    (2,5),\n",
    "    (3,7),\n",
    "    (4,8),\n",
    "    (4,4),\n",
    "    (6,3),\n",
    "    (7,2),\n",
    "    (8,5),\n",
    "    (8,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the points in a NumPy table.\n",
    "data = np.array(points)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data.\n",
    "plt.scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the weights to (0,0) for a baseline\n",
    "weights = np.array([0,0])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line over the interval [0,10] if you know w0 and w1 for the line.\n",
    "def plotline(w0=weights[0], w1=weights[1], color='r'):\n",
    "    x0  = 0\n",
    "    y0  = w0\n",
    "    x10 = 10\n",
    "    y10 = w0 + w1 * 10\n",
    "    plt.plot([x0,x10], [y0, y10], color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input data from their targets.\n",
    "# All but the last column are input data.\n",
    "X = data[:, :1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last column contains the target values.\n",
    "Tar = data[:, 1:]\n",
    "Tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the weights into a column vector, a (n+1)x1 matrix\n",
    "# n is the number of features for a data point.\n",
    "W = weights.reshape(-1,1)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construt bias as 1-s.\n",
    "Bias = np.ones((X.shape[0], 1))\n",
    "Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the bias into position 0 of the input data.\n",
    "# Observe the argument axis=1\n",
    "Xb = np.concatenate((Bias, X), axis=1)\n",
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember earlier weights when we update.\n",
    "old_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate\n",
    "eta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat what the data and baseline looks like.\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Xb @ W\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Y - Tar\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grad = Xb.T @ D\n",
    "Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights.append((W[0,0], W[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W - eta * Grad\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result of updating the weights.\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate\n",
    "Return to the headline *Gradient descent* and run again the steps from there to here and see the change.\n",
    "\n",
    "Repeat 3-4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (20):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (200):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    Y = Xb @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xb.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for i in range(0,2000,200):\n",
    "    ws = old_weights[i]\n",
    "    plotline(ws[0], ws[1], 'g')\n",
    "plotline(W[0,0], W[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The learned weigths, w0: {:10.7f}, w1: {:10.7f}\".format(W[0,0], W[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "You may experiment with larger learning rates, eg., 0.005 or 0.01.\n",
    "\n",
    "You wil probably want to stop the training at some point when the results are satisfactory.\n",
    "More on that in the weekly sets 7 and 8 and Mandatory 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest was not part of last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "The training is slow. If we try we a much bigger training rate, it does not converge. Let us try scaling. We will use a min-max scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = np.max(X)\n",
    "x_min = np.min(X)\n",
    "X_scaled = (X - x_min)/(x_max - x_min)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias\n",
    "Bias = np.ones((X_scaled.shape[0], 1))\n",
    "Xs = np.concatenate((Bias, X_scaled), axis=1)\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line over the interval [0,10] if you know w0 and w1 for the line.\n",
    "def plotline_scaled(w0=weights[0], w1=weights[1], xmax=1, xmin=0, color='r'):\n",
    "    x0  = 0\n",
    "    y0  = w0 + w1*(0-xmin)/(xmax-xmin)\n",
    "    x10 = 10\n",
    "    y10 = w0 + w1 * (10-xmin)/(xmax-xmin)\n",
    "    plt.plot([x0,x10], [y0, y10], color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat what the data and baseline looks like.\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline_scaled(xmax=x_max, xmin=x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate\n",
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reset the weights\n",
    "W = np.zeros((2,1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    Y = Xs @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xs.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline_scaled(ws[0], ws[1], xmax=x_max, xmin=x_min, color='g')\n",
    "plotline_scaled(W[0,0], W[1,0], xmax=x_max, xmin=x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (100):\n",
    "    Y = Xs @ W\n",
    "    D = Y - Tar\n",
    "    Grad = Xs.T @ D\n",
    "    old_weights.append((W[0,0], W[1,0]))\n",
    "    W = W - eta * Grad\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "for ws in old_weights:\n",
    "    plotline_scaled(ws[0], ws[1], xmax=x_max, xmin=x_min, color='g')\n",
    "plotline_scaled(W[0,0], W[1,0], xmax=x_max, xmin=x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,101,50):\n",
    "    print(old_weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour lines\n",
    "Back to the unscaled data set. We will try to plot some contour lines.\n",
    "(Hopefully my highschool maths isn't too rusty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datab = np.concatenate((Xb,Tar), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = datab.T @ datab\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = coeffs[0,0]\n",
    "b = coeffs[0,1] + coeffs[1,0]\n",
    "c = - coeffs[0,2] - coeffs[2,0]\n",
    "d = coeffs[1,1]\n",
    "e = - coeffs[1,2] - coeffs[2,1]\n",
    "f = coeffs[2,2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(w_0 + x_1w_1 - t)^2$\n",
    "\n",
    "This yields the expression\n",
    "\n",
    "$a w_0^2 + bw_0w_1 + c w_0  + d w_1^2 + e w_1  + f $\n",
    "\n",
    "We will consider when this equals $r$ for various $r$-s. Solving with respect $w_0$.\n",
    "\n",
    "$a w_0^2 + w_0(b w_1 + c) + d w_1^2 + e w_1 + f - r$\n",
    "\n",
    "$w0 = (-(b w_1 + c ) +- ((b w_1 + c)^2 - 4 a (d w_1^2 + e w_1  + f - r))^{0.5})/2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20, 20, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range (40,200,40):\n",
    "    xn = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for w1 in x:\n",
    "        A = a\n",
    "        B = b*w1 + c\n",
    "        C = d * w1**2 + e * w1 + f - r\n",
    "        under = B**2 - 4 * A * C\n",
    "        if under > 0:\n",
    "            xn.append(w1)\n",
    "            r1 = (-B + under**0.5)/(2*A)\n",
    "            r2 = (-B - under**0.5)/(2*A)\n",
    "            y1.append(r1)\n",
    "            y2.append(r2)\n",
    "    plt.xlabel(\"w1\")\n",
    "    plt.ylabel(\"w0\")\n",
    "    plt.plot(xn,y1, 'b')\n",
    "    plt.plot(xn,y2, 'b')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With equal scale on the two axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range (40,200,40):\n",
    "    xn = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for w1 in x:\n",
    "        A = a\n",
    "        B = b*w1 + c\n",
    "        C = d * w1**2 + e * w1 + f - r\n",
    "        under = B**2 - 4 * A * C\n",
    "        if under > 0:\n",
    "            xn.append(w1)\n",
    "            r1 = (-B + under**0.5)/(2*A)\n",
    "            r2 = (-B - under**0.5)/(2*A)\n",
    "            y1.append(r1)\n",
    "            y2.append(r2)\n",
    "    plt.xlabel(\"w1\")\n",
    "    plt.ylabel(\"w0\")\n",
    "    plt.plot(xn,y1, 'b')\n",
    "    plt.plot(xn,y2, 'b')\n",
    "    plt.axis('equal')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = np.concatenate((Xs,Tar), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = datas.T @ datas\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = coeffs[0,0]\n",
    "b = coeffs[0,1] + coeffs[1,0]\n",
    "c = - coeffs[0,2] - coeffs[2,0]\n",
    "d = coeffs[1,1]\n",
    "e = - coeffs[1,2] - coeffs[2,1]\n",
    "f = coeffs[2,2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields the expression\n",
    "$a w_0^2 + bw_0w_1 + c w_0  + d w_1^2 + e w_1  + f $\n",
    "\n",
    "We will consider when this equals $r$ for various $r$-s. Solving with respect $w_0$.\n",
    "\n",
    "$a w_0^2 + w_0(b w_1 + c) + d w_1^2 + e w_1 + f - r$\n",
    "\n",
    "$w0 = (-(b w_1 + c ) +- ((b w_1 + c)^2 - 4 a (d w_1^2 + e w_1  + f - r))^{0.5})/2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20, 20, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range (40,200,40):\n",
    "    xn = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for w1 in x:\n",
    "        A = a\n",
    "        B = b*w1 + c\n",
    "        C = d * w1**2 + e * w1 + f - r\n",
    "        under = B**2 - 4 * A * C\n",
    "        if under > 0:\n",
    "            xn.append(w1)\n",
    "            r1 = (-B + under**0.5)/(2*A)\n",
    "            r2 = (-B - under**0.5)/(2*A)\n",
    "            y1.append(r1)\n",
    "            y2.append(r2)\n",
    "    plt.xlabel(\"w1\")\n",
    "    plt.ylabel(\"w0\")\n",
    "    plt.plot(xn,y1, 'b')\n",
    "    plt.plot(xn,y2, 'b')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With equal scale on the two axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range (40,200,40):\n",
    "    xn = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    for w1 in x:\n",
    "        A = a\n",
    "        B = b*w1 + c\n",
    "        C = d * w1**2 + e * w1 + f - r\n",
    "        under = B**2 - 4 * A * C\n",
    "        if under > 0:\n",
    "            xn.append(w1)\n",
    "            r1 = (-B + under**0.5)/(2*A)\n",
    "            r2 = (-B - under**0.5)/(2*A)\n",
    "            y1.append(r1)\n",
    "            y2.append(r2)\n",
    "    plt.plot(xn,y1, 'b')\n",
    "    plt.plot(xn,y2, 'b')\n",
    "    plt.xlabel(\"w1\")\n",
    "    plt.ylabel(\"w0\")\n",
    "    plt.axis('equal')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you find this part difficult, you may skip it and move to the rest of the exercise set which does not depend of you doing this part.)\n",
    "\n",
    "We mentioned in the lectures that the linear regression problem has a closed-form solution. Say that we have a linear regression problem where we try to predict $y$ from $x_1, x_2, \\ldots,x_n$ by a linear formula  $f(\\mathbf{x})=w_0+w_1x_1+\\cdots+w_nx_n$ on the basis of $N$ observations of the form $(\\mathbf{x}_i, t_i)$.\n",
    "If we extend each observation with a bias $x_0=1$ and put the $\\mathbf{x}_i$-s into a \n",
    "$(N\\times(n+1))$ matrix, and the $t_i$-s into a $N\\times 1$ column matrix $Y$, as we have done above, then we can find the weights (coeffecients/parameters) by the formula\n",
    "$W =\\theta = (X^T X)^{-1}X^TY$.\n",
    " \n",
    "Some explanation to the formula. A square matrix $A$ is called the *identity matrix* if $A[i,i]=1$ for all $i$, and $A[i,j]=0$ if $i\\neq j$. We use $I$ as a name of this matrix. It is called the *identity matrix* because $AI=IA=A$ for all $A$.\n",
    "\n",
    "Given a square matrix $A$. If there exists a matrix $B$, such that $AB=BA=I$, we will say that $B$ is the *inverse* of $A$ and write this $B=A^{-1}$. Not all matrices have inverses.\n",
    "\n",
    "In NumPy, the function `np.linalg.inv(X)` yields the inverse of `X` if it exists.\n",
    "\n",
    "We wil not try to explain why the formula $\\theta = (X^T X)^{-1}X^TY$ yields the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(Xb.T @ Xb) @ Xb.T @ Tar\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1])\n",
    "plotline(beta[0,0], beta[1,0], 'g')\n",
    "# plotline(W[0,0], W[1,0])\n",
    "plotline_scaled(W[0,0], W[1,0], xmax=x_max, xmin=x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
